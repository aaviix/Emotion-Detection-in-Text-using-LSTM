{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WwC4Ih1zHMkc"
      },
      "source": [
        "**Emotion Detection in Tweets - LSTM Model from Scratch**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xSnMyqQRHQIW"
      },
      "source": [
        "# SETUP for Colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uRfEs6fXHCyX"
      },
      "outputs": [],
      "source": [
        "!pip install wordcloud"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sICiWdXqHaS5"
      },
      "source": [
        "# Import **Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x1pH32PiHbXU"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import re\n",
        "import string\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.manifold import TSNE\n",
        "from wordcloud import WordCloud\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OA6HDZmqHmcC"
      },
      "source": [
        "# Load Dataset (Upload Files)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386
        },
        "id": "mS2bpNW7HonO",
        "outputId": "723066e1-b1b9-4cbc-cd62-09f8c4ba3b6c"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-d218a69a-7a66-4272-8c3f-012b5f05e357\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-d218a69a-7a66-4272-8c3f-012b5f05e357\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "TypeError",
          "evalue": "'NoneType' object is not subscriptable",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-88c68a3664ee>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0muploaded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Load train / val / test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtrain_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/train.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m';'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36mupload\u001b[0;34m(target_dir)\u001b[0m\n\u001b[1;32m     70\u001b[0m   \"\"\"\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m   \u001b[0muploaded_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_upload_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmultiple\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m   \u001b[0;31m# Mapping from original filename to filename as saved locally.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m   \u001b[0mlocal_filenames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36m_upload_files\u001b[0;34m(multiple)\u001b[0m\n\u001b[1;32m    169\u001b[0m   \u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_collections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m   \u001b[0;32mwhile\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'action'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'complete'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m     result = _output.eval_js(\n\u001b[1;32m    173\u001b[0m         'google.colab._files._uploadFilesContinue(\"{output_id}\")'.format(\n",
            "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Load train / val / test\n",
        "train_df = pd.read_csv('/content/train.txt', sep=';', names=['text', 'label'])\n",
        "val_df = pd.read_csv('/content/val.txt', sep=';', names=['text', 'label'])\n",
        "test_df = pd.read_csv('/content/test.txt', sep=';', names=['text', 'label'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oiduOs1gH5yO"
      },
      "source": [
        "# EDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7eY_-24SH9L5"
      },
      "outputs": [],
      "source": [
        "# Number of samples\n",
        "print(f\"Train samples: {len(train_df)}\")\n",
        "print(f\"Validation samples: {len(val_df)}\")\n",
        "print(f\"Test samples: {len(test_df)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Vy7EIGsIFNx"
      },
      "outputs": [],
      "source": [
        "# Unique classes\n",
        "print(\"Classes:\", train_df['label'].unique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3_G7ZmNCYHju"
      },
      "outputs": [],
      "source": [
        "# Unique classes\n",
        "print(\"Classes:\", test_df['label'].unique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LgW_rCWyYJ0w"
      },
      "outputs": [],
      "source": [
        "# Unique classes\n",
        "print(\"Classes:\", val_df['label'].unique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CXF6vGJsIHlr"
      },
      "outputs": [],
      "source": [
        "# Class balance\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.countplot(data=train_df, x='label', order=train_df['label'].value_counts().index)\n",
        "plt.title('Class Distribution (Train)')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yAI_XinQXh6_"
      },
      "outputs": [],
      "source": [
        "# Class balance\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.countplot(data=test_df, x='label', order=test_df['label'].value_counts().index)\n",
        "plt.title('Class Distribution (Test)')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2s5UhAe3XnW6"
      },
      "outputs": [],
      "source": [
        "# Class balance\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.countplot(data=val_df, x='label', order=val_df['label'].value_counts().index)\n",
        "plt.title('Class Distribution (Validation)')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "meaYEi5-IKA5"
      },
      "outputs": [],
      "source": [
        "# Text length distribution\n",
        "train_df['text_length'] = train_df['text'].apply(lambda x: len(x.split()))\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.histplot(train_df['text_length'], bins=30)\n",
        "plt.title('Distribution of Tweet Length (words)')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VUv2x8cBW_k5"
      },
      "outputs": [],
      "source": [
        "# Text length distribution\n",
        "test_df['text_length'] = test_df['text'].apply(lambda x: len(x.split()))\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.histplot(test_df['text_length'], bins=30) # Changed to use test_df and the correct column name\n",
        "plt.title('Distribution of Tweet Length (words)')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yE-F_QEgXN6x"
      },
      "outputs": [],
      "source": [
        "# Validation length distribution\n",
        "# Calculate the text length for the 'text' column in the val_df DataFrame\n",
        "val_df['val_length'] = val_df['text'].apply(lambda x: len(x.split()))\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.histplot(val_df['val_length'], bins=30)\n",
        "plt.title('Distribution of Tweet Length (words)')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4e4xj63LIMf6"
      },
      "outputs": [],
      "source": [
        "# Example per class\n",
        "for label in train_df['label'].unique():\n",
        "    print(f\"\\nLabel: {label}\")\n",
        "    print(train_df[train_df['label'] == label]['text'].sample(1).values[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m5LcvnxPWAo0"
      },
      "outputs": [],
      "source": [
        "# Example per class\n",
        "for label in test_df['label'].unique():\n",
        "    print(f\"\\nLabel: {label}\")\n",
        "    # Access the 'text' column instead of the non-existent 'test' column\n",
        "    print(test_df[test_df['label'] == label]['text'].sample(1).values[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xAzvK-vkWn74"
      },
      "outputs": [],
      "source": [
        "# Example per class\n",
        "for label in val_df['label'].unique():\n",
        "    print(f\"\\nLabel: {label}\")\n",
        "    # Access the 'text' column instead of the non-existent 'test' column\n",
        "    print(val_df[val_df['label'] == label]['text'].sample(1).values[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q8VJusDvIQUQ"
      },
      "outputs": [],
      "source": [
        "# Wordclouds\n",
        "for label in train_df['label'].unique():\n",
        "    text = \" \".join(train_df[train_df['label'] == label]['text'])\n",
        "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.imshow(wordcloud, interpolation='bilinear')\n",
        "    plt.axis('off')\n",
        "    plt.title(f\"Wordcloud for {label}\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1yC1LU_zZP-V"
      },
      "source": [
        "## Checking null and removing Duplicates in train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Orxx95qtZPry"
      },
      "outputs": [],
      "source": [
        "print(train_df.isnull().sum())\n",
        "print(train_df.duplicated().sum())\n",
        "\n",
        "# removing Duplicates\n",
        "index = train_df[train_df.duplicated()==True].index\n",
        "train_df.drop(index , axis =0 ,inplace=True)\n",
        "train_df.reset_index(inplace=True, drop=True)\n",
        "\n",
        "print(train_df.duplicated().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-aIQoOOeZYqt"
      },
      "source": [
        "## Checking null and removing Duplicates in test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IhRO-8a3ZZ2v"
      },
      "outputs": [],
      "source": [
        "print(test_df.isnull().sum())\n",
        "print(test_df.duplicated().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4e-3-qV6Zb3K"
      },
      "source": [
        "## Checking null and removing Duplicates in val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XRpcqQAKZgCH"
      },
      "outputs": [],
      "source": [
        "print(val_df.isnull().sum())\n",
        "print(val_df.duplicated().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uaLJ9QQ0ZmMx"
      },
      "outputs": [],
      "source": [
        "#text lengths\n",
        "train_df['text_len'] = train_df['text'].apply(lambda x: len(x.split()))\n",
        "val_df['text_len'] = val_df['text'].apply(lambda x: len(x.split()))\n",
        "test_df['text_len'] = test_df['text'].apply(lambda x: len(x.split()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hTFQVglCZor2"
      },
      "outputs": [],
      "source": [
        "#Train Set\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.histplot(train_df['text_len'], bins=30, kde=True, color='blue')\n",
        "plt.title(\"Text Length Distribution - Train Set\")\n",
        "plt.xlabel(\"Number of Words\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D1SMDiZ4Zq9A"
      },
      "outputs": [],
      "source": [
        "#Validation Set\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.histplot(val_df['text_len'], bins=30, kde=True, color='orange')\n",
        "plt.title(\"Text Length Distribution - Validation Set\")\n",
        "plt.xlabel(\"Number of Words\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "elwy8yA7ZtM7"
      },
      "outputs": [],
      "source": [
        "# Test Set\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.histplot(test_df['text_len'], bins=30, kde=True, color='green')\n",
        "plt.title(\"Text Length Distribution - Test Set\")\n",
        "plt.xlabel(\"Number of Words\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qnCw_V-MZy68"
      },
      "outputs": [],
      "source": [
        "# Words\n",
        "train_df.groupby('label')['text_len'].mean().sort_values().plot(kind='barh', title=\"Avg Word Count per Emotion\")\n",
        "plt.xlabel(\"Avg Word Count\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eI6PbVoFZ1cQ"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "vectorizer = CountVectorizer(ngram_range=(2, 2), stop_words='english')\n",
        "X = vectorizer.fit_transform(train_df['text'])\n",
        "sum_words = X.sum(axis=0)\n",
        "words_freq = [(word, sum_words[0, idx]) for word, idx in vectorizer.vocabulary_.items()]\n",
        "words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# Plot top 20 bigrams\n",
        "top_20_bigrams = words_freq[:20]\n",
        "bigrams_df = pd.DataFrame(top_20_bigrams, columns=['Bigram', 'Frequency'])\n",
        "sns.barplot(y='Bigram', x='Frequency', data=bigrams_df)\n",
        "plt.title(\"Top 20 Bigrams in Text\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JJB2Mt07Z5xx"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(6, 6))\n",
        "train_df['label'].value_counts().plot.pie(autopct='%1.1f%%', startangle=140, cmap='Set3')\n",
        "plt.title(\"Emotion Distribution Pie Chart\")\n",
        "plt.ylabel('')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F9iwD9QuZ9KR"
      },
      "source": [
        "### identification of outliers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Qkn4r3naAEe"
      },
      "outputs": [],
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "train_df['sentiment_polarity'] = train_df['text'].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
        "\n",
        "sns.boxplot(data=train_df, x='label', y='sentiment_polarity')\n",
        "plt.xticks(rotation=45)\n",
        "plt.title(\"Sentiment Polarity per Emotion\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMc3OGesIYmq"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_glcZzMbQmj"
      },
      "source": [
        "### Comparision of rows in Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5EeRoPc0baQ8"
      },
      "source": [
        "####Define the dataframe_difference function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mEkf7XUQbU_5"
      },
      "outputs": [],
      "source": [
        "def dataframe_difference(df1, df2, which=None):\n",
        "    \"\"\"\n",
        "    Find rows which are different between two DataFrames.\n",
        "    \"\"\"\n",
        "    comparison_df = df1.merge(\n",
        "        df2,\n",
        "        indicator=True,\n",
        "        how='outer'\n",
        "    )\n",
        "    if which is None:\n",
        "        diff_df = comparison_df[comparison_df['_merge'] != 'both']\n",
        "    else:\n",
        "        diff_df = comparison_df[comparison_df['_merge'] == which]\n",
        "    return diff_df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0v9kVE5bc9D"
      },
      "source": [
        "#### function to compare train/val/test sets for overlaps:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FkIx85rubhVX"
      },
      "outputs": [],
      "source": [
        "print(\"Checking overlap between train and test:\")\n",
        "print(dataframe_difference(train_df, test_df, which='both'))\n",
        "\n",
        "print(\"Checking overlap between train and validation:\")\n",
        "print(dataframe_difference(train_df, val_df, which='both'))\n",
        "\n",
        "print(\"Checking overlap between validation and test:\")\n",
        "print(dataframe_difference(val_df, test_df, which='both'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJmO2iFNcP6m"
      },
      "source": [
        "#### Remove overlapping rows from validation and test sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kLtBm8vFcKFA"
      },
      "outputs": [],
      "source": [
        "# Remove overlapping rows in validation\n",
        "overlap_train_val = dataframe_difference(train_df, val_df, which='both')\n",
        "val_df = val_df[~val_df['text'].isin(overlap_train_val['text'])]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R17AUMRxcU6q"
      },
      "outputs": [],
      "source": [
        "# Remove overlapping rows in test\n",
        "overlap_train_test = dataframe_difference(train_df, test_df, which='both')\n",
        "test_df = test_df[~test_df['text'].isin(overlap_train_test['text'])]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YBZcg6e6bk7J"
      },
      "source": [
        "### Cleaning the Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YlSLgWc5dBX6"
      },
      "source": [
        "#### Import NLTK and download required corpora:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Von59__QaZjk"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PioEviX2abN-"
      },
      "outputs": [],
      "source": [
        "# Download NLTK data\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "stop_words = set(stopwords.words('english'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WyDhO8AidFhq"
      },
      "source": [
        "#### Define text cleaning functions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c7fov6kBagFC"
      },
      "outputs": [],
      "source": [
        "def lemmatization(text):\n",
        "    lemmatizer= WordNetLemmatizer()\n",
        "    text = text.split()\n",
        "    text=[lemmatizer.lemmatize(y) for y in text]\n",
        "    return \" \".join(text)\n",
        "\n",
        "def remove_stop_words(text):\n",
        "    Text=[i for i in str(text).split() if i not in stop_words]\n",
        "    return \" \".join(Text)\n",
        "\n",
        "def Removing_numbers(text):\n",
        "    text=''.join([i for i in text if not i.isdigit()])\n",
        "    return text\n",
        "\n",
        "def lower_case(text):\n",
        "    text = text.split()\n",
        "    text=[y.lower() for y in text]\n",
        "    return \" \".join(text)\n",
        "\n",
        "def Removing_punctuations(text):\n",
        "    text = re.sub('[%s]' % re.escape(\"\"\"!\"#$%&'()*+,،-./:;<=>؟?@[\\]^_`{|}~\"\"\"), ' ', text)\n",
        "    text = text.replace('؛',\"\")\n",
        "    text = re.sub('\\s+', ' ', text)\n",
        "    text = \" \".join(text.split())\n",
        "    return text.strip()\n",
        "\n",
        "def Removing_urls(text):\n",
        "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "    return url_pattern.sub(r'', text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swWI_EkddJvu"
      },
      "source": [
        "#### Apply normalization pipeline to train, val, test sets:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SaGnjr_YanFe"
      },
      "outputs": [],
      "source": [
        "def normalize_text(df, col_name):\n",
        "    df[col_name] = df[col_name].apply(lower_case)\n",
        "    df[col_name] = df[col_name].apply(remove_stop_words)\n",
        "    df[col_name] = df[col_name].apply(Removing_numbers)\n",
        "    df[col_name] = df[col_name].apply(Removing_punctuations)\n",
        "    df[col_name] = df[col_name].apply(Removing_urls)\n",
        "    df[col_name] = df[col_name].apply(lemmatization)\n",
        "    return df\n",
        "\n",
        "# Apply normalization\n",
        "train_df = normalize_text(train_df, 'text')\n",
        "val_df = normalize_text(val_df, 'text')\n",
        "test_df = normalize_text(test_df, 'text')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "evtyfdqzIWkn"
      },
      "outputs": [],
      "source": [
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'http\\S+', '', text)\n",
        "    text = re.sub(r'@\\w+', '', text)\n",
        "    text = re.sub(r'#', '', text)\n",
        "    text = re.sub(r'[%s]' % re.escape(string.punctuation), '', text)\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    return text.strip()\n",
        "\n",
        "train_df['clean_text'] = train_df['text'].apply(clean_text)\n",
        "val_df['clean_text'] = val_df['text'].apply(clean_text)\n",
        "test_df['clean_text'] = test_df['text'].apply(clean_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jm-IBNtudYCw"
      },
      "source": [
        "#### Alternative clean_text function (simpler cleaning):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CuxYIyDPdZcy"
      },
      "outputs": [],
      "source": [
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'http\\S+', '', text)\n",
        "    text = re.sub(r'@\\w+', '', text)\n",
        "    text = re.sub(r'#', '', text)\n",
        "    text = re.sub(r'[%s]' % re.escape(string.punctuation), '', text)\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    return text.strip()\n",
        "\n",
        "train_df['clean_text'] = train_df['text'].apply(clean_text)\n",
        "val_df['clean_text'] = val_df['text'].apply(clean_text)\n",
        "test_df['clean_text'] = test_df['text'].apply(clean_text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PkDrY9w1ddFX"
      },
      "source": [
        "#### Initialize tokenizer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MtrnZULdIfFm"
      },
      "outputs": [],
      "source": [
        "MAX_VOCAB = 10000\n",
        "MAX_LEN = 50\n",
        "\n",
        "tokenizer = Tokenizer(num_words=MAX_VOCAB, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(train_df['clean_text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CdatiUGcdidA"
      },
      "source": [
        "#### Convert texts to padded sequences:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PfAKncKlIhL1"
      },
      "outputs": [],
      "source": [
        "X_train = pad_sequences(tokenizer.texts_to_sequences(train_df['clean_text']), maxlen=MAX_LEN)\n",
        "X_val = pad_sequences(tokenizer.texts_to_sequences(val_df['clean_text']), maxlen=MAX_LEN)\n",
        "X_test = pad_sequences(tokenizer.texts_to_sequences(test_df['clean_text']), maxlen=MAX_LEN)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1g0BcJjTdnYP"
      },
      "source": [
        "#### Encode labels to categorical:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n61w6CwcIjVi"
      },
      "outputs": [],
      "source": [
        "label_encoder = LabelEncoder()\n",
        "label_encoder.fit(train_df['label'])\n",
        "\n",
        "y_train = to_categorical(label_encoder.transform(train_df['label']))\n",
        "y_val = to_categorical(label_encoder.transform(val_df['label']))\n",
        "y_test = to_categorical(label_encoder.transform(test_df['label']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dd4qUON1vi9T"
      },
      "source": [
        "# Label Encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XtXoHRdAvnN8"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Initialize LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# Fit and transform labels for all datasets\n",
        "train_labels_encoded = label_encoder.fit_transform(train_df['label'])\n",
        "val_labels_encoded = label_encoder.transform(val_df['label'])\n",
        "test_labels_encoded = label_encoder.transform(test_df['label'])\n",
        "\n",
        "# Convert integer labels to one-hot encoded vectors\n",
        "num_classes = len(label_encoder.classes_)\n",
        "train_labels_one_hot = to_categorical(train_labels_encoded, num_classes=num_classes)\n",
        "val_labels_one_hot = to_categorical(val_labels_encoded, num_classes=num_classes)\n",
        "test_labels_one_hot = to_categorical(test_labels_encoded, num_classes=num_classes)\n",
        "\n",
        "print(\"\\nOriginal labels:\", train_df['label'].head())\n",
        "print(\"Encoded labels:\", train_labels_encoded[:5])\n",
        "print(\"One-hot encoded labels:\", train_labels_one_hot[:5])\n",
        "print(\"Number of classes:\", num_classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dkuV52oxIl2f"
      },
      "source": [
        "# Build Model\n",
        "\n",
        "1. Embedding Layer\n",
        "- Converts each integer word index into a dense vector of fixed size (EMBEDDING_DIM = 100).\n",
        "- These vectors are learned during training (random initialized initially).\n",
        "- Purpose: capture semantic meaning of words in a continuous vector space.\n",
        "\n",
        "2. LSTM Layer\n",
        "- Long Short-Term Memory (LSTM) is a type of Recurrent Neural Network (RNN) specialized for sequence data.\n",
        "- Processes the sequence of word embeddings step-by-step, keeping track of information over time.\n",
        "- Can capture context and dependencies between words, which is critical in understanding emotions in text.\n",
        "- Here, we use 128 units, and return_sequences=False means it outputs the final state vector summarizing the whole tweet.\n",
        "\n",
        "3. Training\n",
        "- Model learns from the training data for 10 epochs.\n",
        "- Batch size of 32: model weights updated every 32 samples.\n",
        "- Validation data used to monitor overfitting and performance on unseen data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ja2Pb_8InTS"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Input\n",
        "\n",
        "model = Sequential([\n",
        "    Input(shape=(max_len,)),\n",
        "    Embedding(vocab_size, embedding_dim),\n",
        "    LSTM(128, return_sequences=True),\n",
        "    LSTM(64),\n",
        "    Dropout(0.5),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "# Now Keras already “knows” the shape, so summary() will be built.\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TMZ2-RAfIvBv"
      },
      "source": [
        "# Train Model\n",
        "\n",
        "## Interpretation of Training Output\n",
        "\n",
        "- The accuracy improves over epochs both on training and validation sets.\n",
        "- The loss decreases steadily, showing the model learns to reduce classification errors.\n",
        "- Watch for signs of overfitting (training accuracy much higher than validation accuracy).\n",
        "- Here, validation accuracy around 92-93% indicates good generalization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZSyyuRYJIs-o"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "print(\"Model compiled successfully!\")\n",
        "\n",
        "# Use the correct variable names X_train and X_val for padded sequences\n",
        "history = model.fit(X_train, train_labels_one_hot,\n",
        "                    epochs=20, # You might need to tune this\n",
        "                    batch_size=32, # You might need to tune this\n",
        "                    validation_data=(X_val, val_labels_one_hot),\n",
        "                    verbose=1)\n",
        "print(\"\\nModel training complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_KOScP2njmJu"
      },
      "source": [
        "# Evaluate the Model (including Evaluate on Test Data, Generate Predictions, and Classification Report)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_9WG4I5zjluK"
      },
      "outputs": [],
      "source": [
        "loss, accuracy = model.evaluate(X_test, test_labels_one_hot, verbose=0)\n",
        "\n",
        "print(f\"\\nTest Loss: {loss:.4f}\")\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zwfw-sUQwie6"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Get predictions\n",
        "predictions_one_hot = model.predict(X_test) # Changed from test_padded to X_test\n",
        "# Convert one-hot predictions back to class labels\n",
        "predictions_encoded = np.argmax(predictions_one_hot, axis=1)\n",
        "\n",
        "# Get true labels (from original encoded test_labels_encoded before one-hot)\n",
        "# test_labels_encoded should be available from your preprocessing step\n",
        "\n",
        "# Convert encoded labels back to original string labels for clarity in report\n",
        "true_labels_str = label_encoder.inverse_transform(test_labels_encoded)\n",
        "predicted_labels_str = label_encoder.inverse_transform(predictions_encoded)\n",
        "\n",
        "# Classification Report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(true_labels_str, predicted_labels_str, target_names=label_encoder.classes_))\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(true_labels_str, predicted_labels_str, labels=label_encoder.classes_)\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=label_encoder.classes_,\n",
        "            yticklabels=label_encoder.classes_)\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ijfODz6jwl-3"
      },
      "outputs": [],
      "source": [
        "# Plot training & validation accuracy values\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model Accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "\n",
        "# Plot training & validation loss values\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model Loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7UZgOBvpI06u"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jM5Iz1AUI2SD"
      },
      "outputs": [],
      "source": [
        "# Accuracy and loss\n",
        "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.legend()\n",
        "plt.title('Accuracy over epochs')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.legend()\n",
        "plt.title('Loss over epochs')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RbwoY9R7I6vP"
      },
      "outputs": [],
      "source": [
        "# Test set evaluation\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "y_true = np.argmax(y_test, axis=1)\n",
        "\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_true, y_pred_classes, target_names=label_encoder.classes_))\n",
        "\n",
        "cm = confusion_matrix(y_true, y_pred_classes)\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
        "plt.title('Confusion Matrix')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GVOSQYuI7da"
      },
      "source": [
        "# Save model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EJYU3nndI-JU"
      },
      "outputs": [],
      "source": [
        "model.save('emotion_lstm_model.h5')\n",
        "print(\"Model saved as emotion_lstm_model.h5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "no0XUGxSJBsh"
      },
      "source": [
        "# t-SNE Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wf7GCHP7JDX1"
      },
      "outputs": [],
      "source": [
        "embedding_layer = model.layers[0]\n",
        "embedding_weights = embedding_layer.get_weights()[0]\n",
        "\n",
        "# Pick first N words\n",
        "N = 300\n",
        "words = list(tokenizer.word_index.keys())[:N]\n",
        "word_embeddings = embedding_weights[1:N+1]\n",
        "\n",
        "tsne = TSNE(n_components=2, perplexity=30, n_iter=1000, random_state=42)\n",
        "embeddings_2d = tsne.fit_transform(word_embeddings)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], alpha=0.7)\n",
        "for i, word in enumerate(words):\n",
        "    plt.annotate(word, (embeddings_2d[i, 0], embeddings_2d[i, 1]))\n",
        "plt.title('t-SNE Visualization of Word Embeddings')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Predictions"
      ],
      "metadata": {
        "id": "xHXRg6zI-AQt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predict_test = np.argmax(model.predict(X_test),axis=-1)"
      ],
      "metadata": {
        "id": "lVNoNE9p-CVO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# as per the dataset description, index value is mapped with the corresponding emotion in the list\n",
        "\n",
        "emotions = ['sadness','joy','love','anger','fear','surprise']"
      ],
      "metadata": {
        "id": "lsHT9mPq-G-W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(n):\n",
        "    # Check if the index n is within the bounds of the test_df DataFrame\n",
        "    if n < len(test_df):\n",
        "        # Use test_df instead of the undefined 'test' variable\n",
        "        print(test_df['text'].iloc[n])\n",
        "        print(\"Actual label: \", emotions[label_encoder.transform([test_df['label'].iloc[n]])[0]])\n",
        "        print(\"Predict label: \",emotions[predict_test[n]])\n",
        "    else:\n",
        "        print(f\"Index {n} is out of bounds for the test dataset.\")"
      ],
      "metadata": {
        "id": "4nBy7csc-JDb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n = 15\n",
        "p = list(np.random.randint(low = 0, high = 2000, size = n))\n",
        "for i in p:\n",
        "    predict(i)\n",
        "    print(\"-------------------------------------------------------------------------------------------\")"
      ],
      "metadata": {
        "id": "kYUEXBE8-66R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_text(test):\n",
        "    txt = tokenizer.texts_to_sequences([test])\n",
        "    txt = pad_sequences(txt,maxlen=128)\n",
        "    pred = np.argmax(model.predict(txt))\n",
        "\n",
        "    return emotions[pred]"
      ],
      "metadata": {
        "id": "F4j5U0xR-Qpn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "txt = '''Giving away something that was of great benefit or of requirement to the receiver definitely\n",
        "brings in a feel of happiness and fulfillment. No matter whatever situation you may be in, when you pass\n",
        "out things that are of great help and happiness to others, you too feel the same.'''\n",
        "\n",
        "print(\"Predicted emotion: \", predict_text(txt))"
      ],
      "metadata": {
        "id": "K980M55K-Rcp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "txt = '''He well remembered the last interview he had had with the old prince at the time of the enrollment,\n",
        "when in reply to an invitation to dinner he had had to listen to an angry\n",
        "reprimand for not having provided his full quota of men.'''\n",
        "\n",
        "print(\"Predicted emotion: \", predict_text(txt))"
      ],
      "metadata": {
        "id": "D1tU32PQ-T1P"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}